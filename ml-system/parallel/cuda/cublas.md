<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [cuBLAS](#cublas)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# cuBLAS

cuBLAS stands for CUDA Basic Linear Algebra Subprograms, it is a fast GPU-accelerated implementation
of the standard [BLAS](http://www.netlib.org/blas/) (a total of 152 BLAS routines). One of the most
important features of cuBLAS is the optimized GEMMs and GEMM extensions for tensore cores. It is
used in many deep learning frameworks like TensorFlow, PyTorch, etc.

The cuBLAS library is freely available as part of the CUDA Toolkit and OpenACC Toolkit.

*References*

- https://docs.nvidia.com/cuda/cublas/index.html
